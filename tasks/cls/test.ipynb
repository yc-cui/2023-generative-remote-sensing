{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from glob import glob\n",
    "import evaluate\n",
    "import transformers\n",
    "\n",
    "transformers.logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/data/cyc/2023-generative-remote-sensing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_custom_dataset(flists):\n",
    "    # read all files and concatenate in a list\n",
    "    lines = []\n",
    "    for file in flists:\n",
    "        with open(file, \"r\") as f:\n",
    "            lines += f.readlines()\n",
    "\n",
    "    # split by ','\n",
    "    lines = [line.strip().split(',') for line in lines]\n",
    "\n",
    "    # create dictionary\n",
    "    dataset_dict = {\"image\": [line[0] for line in lines], \"label\": [int(line[1]) for line in lines]}\n",
    "\n",
    "    # create dataset\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = load_custom_dataset([\"datasets/new/AID_test0.2/new.flist\", \"datasets/data/cls/AID_test0.2/train.flist\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = load_custom_dataset([\"datasets/data/cls/AID_test0.2/test.flist\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset food101 (/home/dell/.cache/huggingface/datasets/food101/default/0.0.0/7cebe41a80fb2da3f08fcbef769c8874073a86346f7fb96dc0847d4dfc318295)\n"
     ]
    }
   ],
   "source": [
    "food = load_dataset(\"food101\", split=\"train[:5000]\")\n",
    "food = food.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = glob(\"datasets/decomp/AID/*/*\")\n",
    "target_list = list(map(lambda x: x.split(\"/\")[-2], data_list))\n",
    "unique_lst = np.unique(target_list)\n",
    "id2label = {str(index) : value for index, value in enumerate(unique_lst)}\n",
    "label2id = {value : str(index) for index, value in enumerate(unique_lst)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /home/dell/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/7cbdb7ee3a6bcdf99dae654893f66519c480a0f8/preprocessor_config.json\n",
      "loading configuration file config.json from cache at /home/dell/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/7cbdb7ee3a6bcdf99dae654893f66519c480a0f8/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.29.1\"\n",
      "}\n",
      "\n",
      "size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "if \"height\" in image_processor.size:\n",
    "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "    crop_size = size\n",
    "    max_size = None\n",
    "elif \"shortest_edge\" in image_processor.size:\n",
    "    size = image_processor.size[\"shortest_edge\"]\n",
    "    crop_size = (size, size)\n",
    "    max_size = image_processor.size.get(\"longest_edge\")\n",
    "\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(crop_size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(crop_size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_train(examples):\n",
    "    examples[\"pixel_values\"] = [train_transforms(Image.open(img).convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "def preprocess_val(examples):\n",
    "    examples[\"pixel_values\"] = [val_transforms(Image.open(img).convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "train_set = train_set.with_transform(preprocess_train)\n",
    "test_set = test_set.with_transform(preprocess_val)\n",
    "\n",
    "\n",
    "\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "f_metric = evaluate.load(\"f1\")\n",
    "r_metric = evaluate.load('recall')\n",
    "p_metric = evaluate.load('precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    pred = np.argmax(logits, axis=1)\n",
    "    acc_at_5 = sum([l in p for l, p in zip(labels, np.argsort(-logits)[:,0:5])]) / len(labels)\n",
    "    acc = acc_metric.compute(predictions=pred, references=labels)\n",
    "    f1_macro = f_metric.compute(predictions=pred, references=labels, average=\"macro\")\n",
    "    f1_micro = f_metric.compute(predictions=pred, references=labels, average=\"micro\")\n",
    "    f1_weighted = f_metric.compute(predictions=pred, references=labels, average=\"weighted\")\n",
    "    r_macro = r_metric.compute(predictions=pred, references=labels, average=\"macro\")\n",
    "    r_micro = r_metric.compute(predictions=pred, references=labels, average=\"micro\")\n",
    "    r_weighted = r_metric.compute(predictions=pred, references=labels, average=\"weighted\")\n",
    "    p_macro = p_metric.compute(predictions=pred, references=labels, average=\"macro\")\n",
    "    p_micro = p_metric.compute(predictions=pred, references=labels, average=\"micro\")\n",
    "    p_weighted = p_metric.compute(predictions=pred, references=labels, average=\"weighted\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc[\"accuracy\"],\n",
    "        \"accuracy@5\": acc_at_5,\n",
    "        \"f1_macro\": f1_macro[\"f1\"],\n",
    "        \"f1_micro\": f1_micro[\"f1\"],\n",
    "        \"f1_weighted\": f1_weighted[\"f1\"],\n",
    "        \"r_macro\": r_macro[\"recall\"],\n",
    "        \"r_micro\": r_micro[\"recall\"],\n",
    "        \"r_weighted\": r_weighted[\"recall\"],\n",
    "        \"p_macro\": p_macro[\"precision\"],\n",
    "        \"p_micro\": p_micro[\"precision\"],\n",
    "        \"p_weighted\": p_weighted[\"precision\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/dell/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/7cbdb7ee3a6bcdf99dae654893f66519c480a0f8/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Airport\",\n",
      "    \"1\": \"BareLand\",\n",
      "    \"10\": \"Farmland\",\n",
      "    \"11\": \"Forest\",\n",
      "    \"12\": \"Industrial\",\n",
      "    \"13\": \"Meadow\",\n",
      "    \"14\": \"MediumResidential\",\n",
      "    \"15\": \"Mountain\",\n",
      "    \"16\": \"Park\",\n",
      "    \"17\": \"Parking\",\n",
      "    \"18\": \"Playground\",\n",
      "    \"19\": \"Pond\",\n",
      "    \"2\": \"BaseballField\",\n",
      "    \"20\": \"Port\",\n",
      "    \"21\": \"RailwayStation\",\n",
      "    \"22\": \"Resort\",\n",
      "    \"23\": \"River\",\n",
      "    \"24\": \"School\",\n",
      "    \"25\": \"SparseResidential\",\n",
      "    \"26\": \"Square\",\n",
      "    \"27\": \"Stadium\",\n",
      "    \"28\": \"StorageTanks\",\n",
      "    \"29\": \"Viaduct\",\n",
      "    \"3\": \"Beach\",\n",
      "    \"4\": \"Bridge\",\n",
      "    \"5\": \"Center\",\n",
      "    \"6\": \"Church\",\n",
      "    \"7\": \"Commercial\",\n",
      "    \"8\": \"DenseResidential\",\n",
      "    \"9\": \"Desert\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Airport\": \"0\",\n",
      "    \"BareLand\": \"1\",\n",
      "    \"BaseballField\": \"2\",\n",
      "    \"Beach\": \"3\",\n",
      "    \"Bridge\": \"4\",\n",
      "    \"Center\": \"5\",\n",
      "    \"Church\": \"6\",\n",
      "    \"Commercial\": \"7\",\n",
      "    \"DenseResidential\": \"8\",\n",
      "    \"Desert\": \"9\",\n",
      "    \"Farmland\": \"10\",\n",
      "    \"Forest\": \"11\",\n",
      "    \"Industrial\": \"12\",\n",
      "    \"Meadow\": \"13\",\n",
      "    \"MediumResidential\": \"14\",\n",
      "    \"Mountain\": \"15\",\n",
      "    \"Park\": \"16\",\n",
      "    \"Parking\": \"17\",\n",
      "    \"Playground\": \"18\",\n",
      "    \"Pond\": \"19\",\n",
      "    \"Port\": \"20\",\n",
      "    \"RailwayStation\": \"21\",\n",
      "    \"Resort\": \"22\",\n",
      "    \"River\": \"23\",\n",
      "    \"School\": \"24\",\n",
      "    \"SparseResidential\": \"25\",\n",
      "    \"Square\": \"26\",\n",
      "    \"Stadium\": \"27\",\n",
      "    \"StorageTanks\": \"28\",\n",
      "    \"Viaduct\": \"29\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.29.1\"\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/dell/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/7cbdb7ee3a6bcdf99dae654893f66519c480a0f8/pytorch_model.bin\n",
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(label2id.keys()),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"cls\"\n",
    "os.environ[\"WANDB_DIR\"] = \"tasks/logs/cls\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_MODE=offline\n",
      "env: WANDB_PROJECT=cls\n",
      "env: WANDB_DIR=tasks/logs/cls\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "# %env WANDB_MODE=offline\n",
    "# %env WANDB_PROJECT=cls\n",
    "# %env WANDB_DIR=tasks/logs/cls\n",
    "# %env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"tasks/logs/cls\", exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"tasks/logs/cls\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/miniconda3/envs/Extend/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 18,000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 210\n",
      "  Number of trainable parameters = 85,821,726\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 27/210 01:17 < 09:28, 0.32 it/s, Epoch 0.37/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
